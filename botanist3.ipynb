{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEHYvswoGL8C",
        "outputId": "29e597a7-af9b-4c6c-bb6c-9073f07bfb47"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn.model_selection as skms\n",
        "import sklearn.preprocessing as skp\n",
        "import sklearn.utils as sku\n",
        "import sklearn.decomposition as skd\n",
        "import sklearn.metrics as skm\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "!pip install umap-learn\n",
        "import umap.umap_ as umap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2ymtD2iGL8D",
        "outputId": "efce2e4a-adb5-458a-b7d3-42f0b40bc017"
      },
      "outputs": [],
      "source": [
        "# GOOGLE COLAB\n",
        "USE_GOOGLE_COLAB = True # Are you using Google Colab ?\n",
        "COLAB_WORKING_PATH = \"/content/drive/My Drive/Colab/Botanist\" # Path to folder in Google Drive\n",
        "\n",
        "# Mount on Google Drive\n",
        "if USE_GOOGLE_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# PATHS\n",
        "DATASET_ZIP_PATH = f\"{COLAB_WORKING_PATH}/herbier.zip\" # Path to zipped data\n",
        "DATASET_PATH = \"/content/data/\" # Where the unzipped data should land ?\n",
        "\n",
        "WORD_DATA_PATH = f\"{DATASET_PATH}/herbier/data_public/words/\"\n",
        "METADATA_PATH = f\"{DATASET_PATH}/herbier/data_public/ascii/words.txt\"\n",
        "\n",
        "# CNN\n",
        "IMAGE_HEIGHT = 50\n",
        "IMAGE_WIDTH = 75\n",
        "\n",
        "CLASSES = ['a01-000u', 'c03-000a']\n",
        "N_CLASSES = len(CLASSES)\n",
        "\n",
        "FLATTEN_LAYER_NAME = 'flattened'\n",
        "\n",
        "DEBUG = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8FDdkIYHAQg"
      },
      "outputs": [],
      "source": [
        "# Check if data is already here\n",
        "if not os.path.exists(DATASET_ZIP_PATH):\n",
        "  !mkdir $DATASET_PATH\n",
        "  !unzip \"$DATASET_ZIP_PATH\" -d $DATASET_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcgXxtSGGL8E"
      },
      "source": [
        "## TODO\n",
        "- cross-validation\n",
        "-\n",
        "- voir pour ignorer le fichier METADATA\n",
        "- prétraitement image\n",
        "  - réduire spectre pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "ZEtOZCZyGL8G",
        "outputId": "22b22e56-1af2-45a0-a50b-d90739dc469e"
      },
      "outputs": [],
      "source": [
        "def load_words_data(data_path, metadata_path, selected_writers = []):\n",
        "    if selected_writers == []:\n",
        "        raise ValueError(\"selected_writers must be a non-empty list of writer IDs\")\n",
        "\n",
        "    data = []\n",
        "\n",
        "    with open(METADATA_PATH, 'r') as file:\n",
        "        for line in file:\n",
        "            if not line.startswith(\"#\"):\n",
        "                components = line.strip().split(' ')\n",
        "                word_id = components[0]\n",
        "\n",
        "                parts = word_id.split('-')\n",
        "                writer_id = '-'.join(parts[:2])\n",
        "\n",
        "                if writer_id in selected_writers:\n",
        "                    image_subfolder = parts[0]\n",
        "                    image_filename = f\"{word_id}.png\"\n",
        "                    image_path = os.path.join(data_path, image_subfolder, writer_id, image_filename)\n",
        "\n",
        "                    if os.path.exists(image_path):\n",
        "                        try:\n",
        "                            img = tf.io.read_file(image_path)\n",
        "                            img = tf.image.decode_png(img)\n",
        "                            data.append({\n",
        "                                'image_path': image_path,\n",
        "                                'writer_id': writer_id,\n",
        "                                'image_array': img\n",
        "                            })\n",
        "                        except tf.errors.InvalidArgumentError:\n",
        "                            print(f\"Image not found for word ID: {word_id} at {image_path}\")\n",
        "                    else:\n",
        "                        print(f\"Image not found for word ID: {word_id} at {image_path}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "words_data = load_words_data(WORD_DATA_PATH, METADATA_PATH, selected_writers=CLASSES)\n",
        "images = [entry['image_array'] for entry in words_data]\n",
        "labels = [entry['writer_id'] for entry in words_data]\n",
        "\n",
        "def plot_images(images, labels, num=10):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(num):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
        "        plt.xlabel(labels[i])\n",
        "    plt.show()\n",
        "\n",
        "if DEBUG:\n",
        "  print(f\"Loaded {len(words_data)} words.\")\n",
        "  for entry in words_data[:5]:\n",
        "      print(f\"  Writer ID: {entry['writer_id']}; image shape: {entry['image_array'].shape}\")\n",
        "\n",
        "  print(\"number of writers: \", len(set([entry['writer_id'] for entry in words_data])))\n",
        "\n",
        "  plot_images(images, labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "8qc3T6jGGL8H",
        "outputId": "e27cc03e-a5f9-4ea9-baa7-738a2d9107e0"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data):\n",
        "    labels = []\n",
        "    images = []\n",
        "\n",
        "    for entry in data:\n",
        "        # Resize the image while preserving aspect ratio\n",
        "        img = np.array(entry['image_array'])\n",
        "        old_size = img.shape[:2]\n",
        "\n",
        "        ratio = float(IMAGE_HEIGHT)/old_size[0]\n",
        "        new_size = tuple([int(x*ratio) for x in old_size])\n",
        "\n",
        "        img = cv2.resize(img, (new_size[1], new_size[0]))\n",
        "\n",
        "        # Ignore images that are too narrows\n",
        "        if new_size[1] < IMAGE_WIDTH:\n",
        "          continue;\n",
        "\n",
        "        # Crop images that are too wide\n",
        "        if new_size[1] > IMAGE_WIDTH:\n",
        "            start_x = (new_size[1] - IMAGE_WIDTH) // 2\n",
        "            img = img[:, start_x:start_x + IMAGE_WIDTH]\n",
        "            new_size = (new_size[0], IMAGE_WIDTH)\n",
        "\n",
        "        # Add padding\n",
        "        \"\"\" This is not relevant anymore since the images are cropped instead\n",
        "        delta_w = IMAGE_WIDTH - new_size[1]\n",
        "        delta_h = IMAGE_HEIGHT - new_size[0]\n",
        "        top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
        "        left, right = delta_w//2, delta_w-(delta_w//2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT)\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        img = img.astype('float32') / 255.0\n",
        "        images.append(img)\n",
        "        labels.append(entry['writer_id'])\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "\n",
        "images, labels = preprocess_data(words_data)\n",
        "X_train, X_test, y_train, y_test = skms.train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "if DEBUG:\n",
        "  print(f\"X_train: {X_train.shape}; y_train: {y_train.shape}\")\n",
        "  print(f\"X_test: {X_test.shape}; y_test: {y_test.shape}\")\n",
        "  plot_images(images, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc3Ewpq7GL8H",
        "outputId": "5dd3f3d0-01aa-4bf2-e25f-c14688c1d85d"
      },
      "outputs": [],
      "source": [
        "# encode labels\n",
        "label_encoder = skp.LabelEncoder()\n",
        "integer_encoded_labels = label_encoder.fit_transform(labels)\n",
        "one_hot_encoded_labels = keras.utils.to_categorical(integer_encoded_labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = skms.train_test_split(images, one_hot_encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Add extra dimension for channel -> 1: black & white)\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "integer_class_labels = np.argmax(y_train, axis=1)\n",
        "\n",
        "if DEBUG:\n",
        "    print(f\"X_train: {X_train.shape}; y_train: {y_train.shape}\")\n",
        "    print(f\"X_test: {X_test.shape}; y_test: {y_test.shape}\")\n",
        "    # number of training samples per class\n",
        "    print(\"number of training samples per class: \", np.bincount(integer_class_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yeAO6p0GL8I"
      },
      "outputs": [],
      "source": [
        "# data augmentation\n",
        "\n",
        "data_generator = keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=False,\n",
        "    fill_mode='nearest'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZCcJcbB3diu",
        "outputId": "ba6ae57e-aa90-4b04-d11a-656af9b89f63"
      },
      "outputs": [],
      "source": [
        "input_layer = keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 1))\n",
        "\n",
        "# Define L1 and L2 regularization\n",
        "l1_l2 = keras.regularizers.l1_l2(l1=0, l2=1e-4)\n",
        "\n",
        "# path 1\n",
        "conv1_1 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l1_l2)(input_layer)\n",
        "pool1_1 = keras.layers.MaxPooling2D((2, 2))(conv1_1)\n",
        "conv1_2 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l1_l2)(pool1_1)\n",
        "pool1_2 = keras.layers.MaxPooling2D((2, 2))(conv1_2)\n",
        "\n",
        "# path 2\n",
        "conv2_1 = keras.layers.Conv2D(32, (5, 5), activation='relu', padding='same', kernel_regularizer=l1_l2)(input_layer)\n",
        "pool2_1 = keras.layers.MaxPooling2D((2, 2))(conv2_1)\n",
        "conv2_2 = keras.layers.Conv2D(64, (5, 5), activation='relu', padding='same', kernel_regularizer=l1_l2)(pool2_1)\n",
        "pool2_2 = keras.layers.MaxPooling2D((2, 2))(conv2_2)\n",
        "\n",
        "# merge paths\n",
        "merged = keras.layers.concatenate([pool1_2, pool2_2])\n",
        "\n",
        "flat = keras.layers.Flatten()(merged)\n",
        "dense1 = keras.layers.Dense(128, activation='relu', kernel_regularizer=l1_l2, name=FLATTEN_LAYER_NAME)(flat)\n",
        "dropout = keras.layers.Dropout(0.2)(dense1)  # Consider experimenting with the dropout rate\n",
        "output_layer = keras.layers.Dense(N_CLASSES, activation='softmax')(dropout)\n",
        "\n",
        "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "if DEBUG:\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk_bxemKGL8K"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lFqwoMcGL8K",
        "outputId": "55a384cb-4121-4204-beb5-c3fc77278288"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "EPOCHS = 20\n",
        "\n",
        "train_generator = data_generator.flow(X_train, y_train, batch_size=BATCH_SIZE)\n",
        "\n",
        "class_weights = sku.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(integer_class_labels),\n",
        "    y=integer_class_labels\n",
        ")\n",
        "class_weights_dict = {i : weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=len(X_train) // BATCH_SIZE,  # Number of batches per epoch\n",
        "    validation_data=(X_test, y_test),\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVt6o-nRGL8M",
        "outputId": "e8d12a93-38c3-4872-8b7e-75d60ec1efc5"
      },
      "outputs": [],
      "source": [
        "feature_layer = model.get_layer(FLATTEN_LAYER_NAME).output\n",
        "\n",
        "# Create a feature extractor model\n",
        "feature_extractor_model = keras.models.Model(inputs=model.input, outputs=feature_layer)\n",
        "\n",
        "# Now you can use this model to extract features\n",
        "features = feature_extractor_model.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YtbceFAVGL8M",
        "outputId": "50175f66-e6ca-439e-ca89-a88a8a136ba2"
      },
      "outputs": [],
      "source": [
        "# features = model.predict(X_train)\n",
        "\n",
        "if DEBUG:\n",
        "    print(f\"features shape: {features.shape}\")\n",
        "\n",
        "# Standardize the features\n",
        "scaler = skp.StandardScaler()\n",
        "features_standardized = scaler.fit_transform(features)\n",
        "\n",
        "# Now, use the standardized features with UMAP\n",
        "def evaluate_model(n_neighbors, min_dist, ax):\n",
        "    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=32, metric='euclidean')\n",
        "    embedding = reducer.fit_transform(np.nan_to_num(features_standardized))\n",
        "\n",
        "    sc = ax.scatter(embedding[:, 0], embedding[:, 1],\n",
        "                    c=integer_class_labels, edgecolor='none', alpha=0.5,\n",
        "                    cmap=plt.cm.get_cmap('Accent', N_CLASSES))\n",
        "    ax.set_xlabel('UMAP component 1')\n",
        "    ax.set_ylabel('UMAP component 2')\n",
        "    ax.set_title(f'n_neighbors={n_neighbors}, min_dist={min_dist}')\n",
        "\n",
        "    if n_neighbors == n_neighbors_list[-1] and min_dist == min_dist_list[-1]:\n",
        "        plt.colorbar(sc, ax=ax)\n",
        "\n",
        "n_neighbors_list = [10, 20, 30]\n",
        "min_dist_list = [0.0, 0.1, 0.2]\n",
        "\n",
        "fig, axes = plt.subplots(len(n_neighbors_list), len(min_dist_list), figsize=(15, 12))\n",
        "\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (n_neighbors, min_dist) in enumerate([(x, y) for x in n_neighbors_list for y in min_dist_list]):\n",
        "    evaluate_model(n_neighbors, min_dist, axes[idx])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
