{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"sN1W8-uZz_qX"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Skipping umap as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: umap-learn in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (0.5.5)\n","Requirement already satisfied: scikit-learn>=0.22 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from umap-learn) (1.2.1)\n","Requirement already satisfied: pynndescent>=0.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from umap-learn) (0.5.11)\n","Requirement already satisfied: scipy>=1.3.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from umap-learn) (1.10.0)\n","Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from umap-learn) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from umap-learn) (1.23.5)\n","Requirement already satisfied: numba>=0.51.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from umap-learn) (0.58.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n","Requirement already satisfied: joblib>=0.11 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from pynndescent>=0.5->umap-learn) (1.1.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from scikit-learn>=0.22->umap-learn) (2.2.0)\n"]}],"source":["!pip uninstall umap -y\n","!pip install umap-learn"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ZAALNQFi0Bcz"},"outputs":[],"source":["USE_GOOGLE_COLAB = False\n","# COLAB_WORKING_PATH = \"/content/drive/My Drive/Colab Notebooks/Botanist\" # Path to folder in Google Drive\n","COLAB_WORKING_PATH = \"/content/drive/My Drive/Colab/Botanist\" if USE_GOOGLE_COLAB else \".\" \n","\n","# PATHS\n","DATASET_ZIP_PATH = COLAB_WORKING_PATH # Path to \"herbier.zip\"\n","DATASET_PATH = \"/content/data/\" # Where the unzipped data should land ?\n","WORD_DATA_PATH = \"/content/data/herbier/data_public/words/\"\n","METADATA_PATH = \"/content/data/herbier/data_public/ascii/words.txt\""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["if USE_GOOGLE_COLAB:\n","  from google.colab import drive\n","  drive.mount('/content/drive/', force_remount=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"JlW8xa2o0DTj"},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: /content: No such file or directory\n","Archive:  ./herbier.zip\n","checkdir:  cannot create extraction directory: /content/data\n","           No such file or directory\n","zsh:cd:1: no such file or directory: /content/data//herbier\n"]}],"source":["# Create our data folder, unzip the data\n","!mkdir $DATASET_PATH\n","!unzip \"$DATASET_ZIP_PATH/herbier.zip\" -d $DATASET_PATH\n","!cd \"$DATASET_PATH/herbier\""]},{"cell_type":"code","execution_count":9,"metadata":{"id":"fdJjewv_uH9l"},"outputs":[],"source":["import os\n","\n","import numpy as np\n","\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import matplotlib.pyplot as plt\n","\n","import sklearn.model_selection as skms\n","import sklearn.preprocessing as skp\n","import sklearn.utils as sku\n","import sklearn.decomposition as skd\n","import sklearn.metrics as skm\n","\n","from sklearn.manifold import TSNE\n","\n","# Module for k-fold\n","from sklearn.model_selection import KFold\n","\n","import umap.umap_ as umap"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Al8QhMtvuH9n"},"outputs":[],"source":["IMAGE_HEIGHT = IMAGE_WIDTH = 128\n","\n","CLASSES = ['a01-000u', 'c03-000a']\n","N_CLASSES = len(CLASSES)\n","\n","FLATTEN_LAYER_NAME = 'flattened'\n","\n","DEBUG = True"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"4BOKlqrvuH9o"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/data/herbier/data_public/ascii/words.txt'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage not found for word ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m---> 37\u001b[0m words_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_words_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWORD_DATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMETADATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_writers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLASSES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG:\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(words_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m words.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[11], line 7\u001b[0m, in \u001b[0;36mload_words_data\u001b[0;34m(data_path, metadata_path, selected_writers)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselected_writers must be a non-empty list of writer IDs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/herbier/data_public/ascii/words.txt'"]}],"source":["def load_words_data(data_path, metadata_path, selected_writers = []):\n","    if selected_writers == []:\n","        raise ValueError(\"selected_writers must be a non-empty list of writer IDs\")\n","\n","    data = []\n","\n","    with open(metadata_path, 'r') as file:\n","        for line in file:\n","            if not line.startswith(\"#\"):\n","                components = line.strip().split(' ')\n","                word_id = components[0]\n","\n","                parts = word_id.split('-')\n","                writer_id = '-'.join(parts[:2])\n","\n","                if writer_id in selected_writers:\n","                    image_subfolder = parts[0]\n","                    image_filename = f\"{word_id}.png\"\n","                    image_path = os.path.join(data_path, image_subfolder, writer_id, image_filename)\n","\n","                    if os.path.exists(image_path):\n","                        try:\n","                            img = tf.io.read_file(image_path)\n","                            img = tf.image.decode_png(img)\n","                            data.append({\n","                                'image_path': image_path,\n","                                'writer_id': writer_id,\n","                                'image_array': img\n","                            })\n","                        except tf.errors.InvalidArgumentError:\n","                            print(f\"Image not found for word ID: {word_id} at {image_path}\")\n","                    else:\n","                        print(f\"Image not found for word ID: {word_id} at {image_path}\")\n","\n","    return data\n","\n","words_data = load_words_data(WORD_DATA_PATH, METADATA_PATH, selected_writers=CLASSES)\n","\n","if DEBUG:\n","  print(f\"Loaded {len(words_data)} words.\")\n","  for entry in words_data[:5]:\n","      print(f\"  Writer ID: {entry['writer_id']}; image shape: {entry['image_array'].shape}\")\n","\n","if DEBUG:\n","    print(\"number of writers: \", len(set([entry['writer_id'] for entry in words_data])))\n","\n","if DEBUG:\n","  plt.figure(figsize=(10, 10))\n","  for i in range(25):\n","      plt.subplot(5, 5, i + 1)\n","      plt.xticks([])\n","      plt.yticks([])\n","      plt.grid(False)\n","      plt.imshow(words_data[i]['image_array'], cmap=plt.cm.binary)\n","      plt.xlabel(words_data[i]['writer_id'])\n","  plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"E006hD9Nxo67"},"source":["**Pre-processing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hulb4sWRuH9p"},"outputs":[],"source":["import cv2\n","\n","def preprocess_data(data):\n","    labels = []\n","    images = []\n","\n","    for entry in data:\n","        # Resize the image while preserving aspect ratio\n","        img = np.array(entry['image_array'])\n","        old_size = img.shape[:2]\n","\n","        ratio = float(IMAGE_HEIGHT)/old_size[0]\n","        new_size = tuple([int(x*ratio) for x in old_size])\n","\n","        img = cv2.resize(img, (new_size[1], new_size[0]))\n","\n","        # Ignore images that are too narrows\n","        if new_size[1] < IMAGE_WIDTH:\n","          continue;\n","\n","        # Crop images that are too wide\n","        if new_size[1] > IMAGE_WIDTH:\n","            start_x = (new_size[1] - IMAGE_WIDTH) // 2\n","            img = img[:, start_x:start_x + IMAGE_WIDTH]\n","            new_size = (new_size[0], IMAGE_WIDTH)\n","\n","        img = img.astype('float32') / 255.0\n","\n","        # Ensure dimensions format is correct: (sample_n, width, height, channels)\n","        img = np.expand_dims(img, axis=-1)\n","        delta_w = IMAGE_WIDTH - new_size[1]\n","        delta_h = IMAGE_HEIGHT - img.shape[0]\n","        delta_w = IMAGE_WIDTH - img.shape[1]\n","        padding = ((0, delta_h), (0, delta_w), (0, 0))\n","        img = np.pad(img, padding, 'constant')\n","\n","        images.append(img)\n","        labels.append(entry['writer_id'])\n","\n","    return np.array(images), np.array(labels)\n","\n","\n","images, labels = preprocess_data(words_data)\n","X_train, X_test, y_train, y_test = skms.train_test_split(images, labels, test_size=0.2, random_state=42)\n","\n","if DEBUG:\n","  print(f\"X_train: {X_train.shape}; y_train: {y_train.shape}\")\n","  print(f\"X_test: {X_test.shape}; y_test: {y_test.shape}\")\n","\n","  num=5\n","  plt.figure(figsize=(10,10))\n","  for i in range(num):\n","      plt.subplot(N_CLASSES,num,i+1)\n","      plt.xticks([])\n","      plt.yticks([])\n","      plt.grid(False)\n","      plt.imshow(images[i], cmap=plt.cm.binary)\n","      plt.xlabel(labels[i])\n","  plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"1IoYwESBxxbc"},"source":["**Kfold**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HvGW_t68xynh"},"outputs":[],"source":["# Define number of splits\n","n_splits = 5\n","\n","# Create Kfold instance\n","kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"WwFl5G6ZyJw0"},"source":["**Set function to use kfold**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mYfw5twWyKTN"},"outputs":[],"source":["def train_and_evaluate(encoder_model, X_train, y_train, X_test, y_test):\n","  BATCH_SIZE = 8 # fine tuned\n","  EPOCHS = 200\n","\n","  class_weights = sku.compute_class_weight(\n","      class_weight='balanced',\n","      classes=np.unique(integer_class_labels),\n","      y=integer_class_labels\n","  )\n","  class_weights_dict = {i : weight for i, weight in enumerate(class_weights)}\n","\n","  early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","  autoencoder.fit(X_train, X_train,  # input and output are the same for an autoencoder\n","                  epochs=EPOCHS,\n","                  batch_size=BATCH_SIZE,\n","                  shuffle=True,\n","                  validation_data=(X_test, X_test))\n","\n","  ############\n","\n","  # Extract features\n","  encoded_features = encoder_model.predict(X_train)\n","\n","  # Standardize the features\n","  scaler = skp.StandardScaler()\n","  encoded_features_standardized = scaler.fit_transform(encoded_features.reshape(len(encoded_features), -1))\n","\n","  # encoded_features_standardized now ready for UMAP\n","\n","  ############\n","\n","  # Standardize the features\n","  # Now, use the standardized features with UMAP\n","\n","  def evaluate_model(n_neighbors, min_dist, ax):\n","      reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=32, metric='euclidean')\n","      embedding = reducer.fit_transform(np.nan_to_num(encoded_features_standardized))\n","\n","      sc = ax.scatter(embedding[:, 0], embedding[:, 1],\n","                      c=integer_class_labels, edgecolor='none', alpha=0.5,\n","                      cmap=plt.cm.get_cmap('Accent', N_CLASSES))\n","      ax.set_xlabel('UMAP component 1')\n","      ax.set_ylabel('UMAP component 2')\n","      ax.set_title(f'n_neighbors={n_neighbors}, min_dist={min_dist}')\n","\n","      if n_neighbors == n_neighbors_list[-1] and min_dist == min_dist_list[-1]:\n","          plt.colorbar(sc, ax=ax)\n","\n","  n_neighbors_list = [10, 20, 30]\n","  min_dist_list = [0.0, 0.1, 0.2]\n","\n","  fig, axes = plt.subplots(len(n_neighbors_list), len(min_dist_list), figsize=(15, 12))\n","\n","  axes = axes.flatten()\n","\n","  for idx, (n_neighbors, min_dist) in enumerate([(x, y) for x in n_neighbors_list for y in min_dist_list]):\n","      evaluate_model(n_neighbors, min_dist, axes[idx])\n","\n","  plt.tight_layout()\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"e10nexn-zD5T"},"source":["**Define model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLenrqdSzEdq"},"outputs":[],"source":["# Encoder\n","input_img = keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 1)) # adapt this if using `channels_first` image data format\n","\n","x = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n","x = keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n","x = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n","x = keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n","x = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n","encoded = keras.layers.MaxPooling2D((2, 2), padding='same', name='encoded_layer')(x)\n","\n","# Decoder\n","x = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n","x = keras.layers.UpSampling2D((2, 2))(x)\n","x = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n","x = keras.layers.UpSampling2D((2, 2))(x)\n","x = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n","x = keras.layers.UpSampling2D((2, 2))(x) # Additional upsampling layer\n","decoded = keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n","\n","\n","# Autoencoder model\n","autoencoder = keras.Model(input_img, decoded)\n","\n","if DEBUG:\n","    autoencoder.summary()\n","\n","autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n","\n","# Create a model to retrieve the encoded features\n","encoder_model = keras.Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('encoded_layer').output)"]},{"cell_type":"markdown","metadata":{"id":"SpFlp0FDzj4Z"},"source":["**Use kfold**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgHsUXjmzka7"},"outputs":[],"source":["# Execute kfold\n","for fold, (train_index, test_index) in enumerate(kf.split(images), 1):\n","\n","    # encode labels\n","    label_encoder = skp.LabelEncoder()\n","    integer_encoded_labels = label_encoder.fit_transform(labels)\n","    one_hot_encoded_labels = keras.utils.to_categorical(integer_encoded_labels)\n","\n","    X_train, X_test = images[train_index], images[test_index]\n","    y_train, y_test = one_hot_encoded_labels[train_index], one_hot_encoded_labels[test_index]\n","\n","    print(f\"\\nFold {fold} - Training set: X_train shape = {X_train.shape}, y_train shape = {y_train.shape}\")\n","    print(f\"Fold {fold} - Testing set: X_test shape = {X_test.shape}, y_test shape = {y_test.shape}\")\n","\n","    integer_class_labels = np.argmax(y_train, axis=1)\n","\n","    # Train and test modele\n","    train_and_evaluate(encoder_model, X_train, y_train, X_test, y_test)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
