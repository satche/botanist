{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JQpTs-iVOHZy"},"outputs":[],"source":["!pip uninstall umap -y\n","!pip install umap-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pU1FJAflPcim"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"]},{"cell_type":"code","source":["COLAB_WORKING_PATH = \"/content/drive/My Drive/Colab Notebooks/Botanist\" # Path to folder in Google Drive\n","\n","# PATHS\n","DATASET_ZIP_PATH = COLAB_WORKING_PATH # Path to \"herbier.zip\"\n","DATASET_PATH = \"/content/data/\" # Where the unzipped data should land ?\n","WORD_DATA_PATH = \"/content/data/herbier/data_public/words/\"\n","METADATA_PATH = \"/content/data/herbier/data_public/ascii/words.txt\""],"metadata":{"id":"kMsQXg2bJZiw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create our data folder, unzip the data\n","!mkdir $DATASET_PATH\n","!unzip \"$DATASET_ZIP_PATH/herbier.zip\" -d $DATASET_PATH\n","!cd \"$DATASET_PATH/herbier\""],"metadata":{"id":"KF2uM4Q9KG1P"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpNYwkAyOHZ0"},"outputs":[],"source":["IMAGE_HEIGHT = IMAGE_WIDTH = 128\n","\n","CLASSES = ['a01-000u', 'c03-000a']\n","N_CLASSES = len(CLASSES)\n","\n","FLATTEN_LAYER_NAME = 'flattened'\n","\n","DEBUG = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HWaDPzKuOHZ0"},"outputs":[],"source":["import os\n","\n","import numpy as np\n","\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import matplotlib.pyplot as plt\n","\n","import sklearn.model_selection as skms\n","import sklearn.preprocessing as skp\n","import sklearn.utils as sku\n","import sklearn.decomposition as skd\n","import sklearn.metrics as skm\n","\n","from sklearn.manifold import TSNE\n","\n","# Module for k-fold\n","from sklearn.model_selection import KFold\n","\n","import umap.umap_ as umap"]},{"cell_type":"markdown","metadata":{"id":"1aIRFzcGOHZ1"},"source":["## TODO\n","- cross-validation\n","-\n","- voir pour ignorer le fichier METADATA\n","- prétraitement image\n","  - réduire spectre pixels\n","  - resize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKRq69r4OHZ2"},"outputs":[],"source":["def load_words_data(data_path, metadata_path, selected_writers = []):\n","    if selected_writers == []:\n","        raise ValueError(\"selected_writers must be a non-empty list of writer IDs\")\n","\n","    data = []\n","\n","    with open(metadata_path, 'r') as file:\n","        for line in file:\n","            if not line.startswith(\"#\"):\n","                components = line.strip().split(' ')\n","                word_id = components[0]\n","\n","                parts = word_id.split('-')\n","                writer_id = '-'.join(parts[:2])\n","\n","                if writer_id in selected_writers:\n","                    image_subfolder = parts[0]\n","                    image_filename = f\"{word_id}.png\"\n","                    image_path = os.path.join(data_path, image_subfolder, writer_id, image_filename)\n","\n","                    if os.path.exists(image_path):\n","                        try:\n","                            img = tf.io.read_file(image_path)\n","                            img = tf.image.decode_png(img)\n","                            data.append({\n","                                'image_path': image_path,\n","                                'writer_id': writer_id,\n","                                'image_array': img\n","                            })\n","                        except tf.errors.InvalidArgumentError:\n","                            print(f\"Image not found for word ID: {word_id} at {image_path}\")\n","                    else:\n","                        print(f\"Image not found for word ID: {word_id} at {image_path}\")\n","\n","    return data\n","\n","words_data = load_words_data(WORD_DATA_PATH, METADATA_PATH, selected_writers=CLASSES)\n","\n","if DEBUG:\n","  print(f\"Loaded {len(words_data)} words.\")\n","  for entry in words_data[:5]:\n","      print(f\"  Writer ID: {entry['writer_id']}; image shape: {entry['image_array'].shape}\")\n","\n","if DEBUG:\n","    print(\"number of writers: \", len(set([entry['writer_id'] for entry in words_data])))\n","\n","if DEBUG:\n","  plt.figure(figsize=(10, 10))\n","  for i in range(25):\n","      plt.subplot(5, 5, i + 1)\n","      plt.xticks([])\n","      plt.yticks([])\n","      plt.grid(False)\n","      plt.imshow(words_data[i]['image_array'], cmap=plt.cm.binary)\n","      plt.xlabel(words_data[i]['writer_id'])\n","  plt.show()\n"]},{"cell_type":"markdown","source":["**Pre-processing**"],"metadata":{"id":"hL1b3PvLTY5B"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JeRayEfpOHZ3"},"outputs":[],"source":["def preprocess_data(data):\n","    labels = []\n","    images = []\n","\n","    for entry in data:\n","        # Resize the image\n","        img = tf.image.resize(entry['image_array'], [IMAGE_HEIGHT, IMAGE_WIDTH])\n","        # TODO: Add [random] paddding\n","\n","        # TODO: divide by average pixel value\n","        img = img.numpy().astype('float32') / 255.0  # Normalize and convert to float32\n","        images.append(img)\n","\n","        labels.append(entry['writer_id'])\n","\n","    return np.array(images), np.array(labels)\n","\n","\n","images, labels = preprocess_data(words_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbSxp-eGOHZ3"},"outputs":[],"source":["# data augmentation\n","\n","data_generator = keras.preprocessing.image.ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=False,\n","    fill_mode='nearest'\n",")"]},{"cell_type":"markdown","source":["**Kfold**"],"metadata":{"id":"j5K7LbvLTgbB"}},{"cell_type":"code","source":["# Define number of splits\n","n_splits = 5\n","\n","# Create Kfold instance\n","kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)"],"metadata":{"id":"gPcfiBsVRxG_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Set function to use kfold**"],"metadata":{"id":"oLk6ZBvzWKd3"}},{"cell_type":"code","source":["def train_and_evaluate(model, X_train, y_train, X_test, y_test):\n","  BATCH_SIZE = 8 # fine tuned\n","  EPOCHS = 20\n","\n","  train_generator = data_generator.flow(X_train, y_train, batch_size=BATCH_SIZE)\n","\n","  class_weights = sku.compute_class_weight(\n","      class_weight='balanced',\n","      classes=np.unique(integer_class_labels),\n","      y=integer_class_labels\n","  )\n","  class_weights_dict = {i : weight for i, weight in enumerate(class_weights)}\n","\n","  early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","  history = model.fit(\n","      train_generator,\n","      epochs=EPOCHS,\n","      steps_per_epoch=len(X_train) // BATCH_SIZE,  # Number of batches per epoch\n","      validation_data=(X_test, y_test),\n","      class_weight=class_weights_dict,\n","      callbacks=[early_stopping]\n","  )\n","\n","  ############\n","\n","  feature_layer = model.get_layer(FLATTEN_LAYER_NAME).output\n","\n","  # Create a feature extractor model\n","  feature_extractor_model = keras.models.Model(inputs=model.input, outputs=feature_layer)\n","\n","  # Now you can use this model to extract features\n","  features = feature_extractor_model.predict(X_train)\n","\n","  ############\n","\n","  # features = model.predict(X_train)\n","\n","  if DEBUG:\n","      print(f\"features shape: {features.shape}\")\n","\n","  # Standardize the features\n","  scaler = skp.StandardScaler()\n","  features_standardized = scaler.fit_transform(features)\n","\n","  # Now, use the standardized features with UMAP\n","\n","  def evaluate_model(n_neighbors, min_dist, ax):\n","      reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=32, metric='euclidean')\n","      embedding = reducer.fit_transform(np.nan_to_num(features_standardized))\n","\n","      sc = ax.scatter(embedding[:, 0], embedding[:, 1],\n","                      c=integer_class_labels, edgecolor='none', alpha=0.5,\n","                      cmap=plt.cm.get_cmap('Accent', N_CLASSES))\n","      ax.set_xlabel('UMAP component 1')\n","      ax.set_ylabel('UMAP component 2')\n","      ax.set_title(f'n_neighbors={n_neighbors}, min_dist={min_dist}')\n","\n","      if n_neighbors == n_neighbors_list[-1] and min_dist == min_dist_list[-1]:\n","          plt.colorbar(sc, ax=ax)\n","\n","  n_neighbors_list = [10, 20, 30]\n","  min_dist_list = [0.0, 0.1, 0.2]\n","\n","  fig, axes = plt.subplots(len(n_neighbors_list), len(min_dist_list), figsize=(15, 12))\n","\n","  axes = axes.flatten()\n","\n","  for idx, (n_neighbors, min_dist) in enumerate([(x, y) for x in n_neighbors_list for y in min_dist_list]):\n","      evaluate_model(n_neighbors, min_dist, axes[idx])\n","\n","  plt.tight_layout()\n","  plt.show()"],"metadata":{"id":"p2--w-RnTsGl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Define model**"],"metadata":{"id":"eygzGbC1ud1n"}},{"cell_type":"code","source":["# Define modele\n","input_layer = keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 1))\n","\n","# Define L1 and L2 regularization\n","l1_l2 = keras.regularizers.l1_l2(l1=0, l2=1e-4)\n","\n","# path 1\n","conv1_1 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l1_l2)(input_layer)\n","pool1_1 = keras.layers.MaxPooling2D((2, 2))(conv1_1)\n","conv1_2 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l1_l2)(pool1_1)\n","pool1_2 = keras.layers.MaxPooling2D((2, 2))(conv1_2)\n","\n","# path 2\n","conv2_1 = keras.layers.Conv2D(32, (5, 5), activation='relu', padding='same', kernel_regularizer=l1_l2)(input_layer)\n","pool2_1 = keras.layers.MaxPooling2D((2, 2))(conv2_1)\n","conv2_2 = keras.layers.Conv2D(64, (5, 5), activation='relu', padding='same', kernel_regularizer=l1_l2)(pool2_1)\n","pool2_2 = keras.layers.MaxPooling2D((2, 2))(conv2_2)\n","\n","# merge paths\n","merged = keras.layers.concatenate([pool1_2, pool2_2])\n","\n","flat = keras.layers.Flatten()(merged)\n","dense1 = keras.layers.Dense(128, activation='relu', kernel_regularizer=l1_l2, name=FLATTEN_LAYER_NAME)(flat)\n","dropout = keras.layers.Dropout(0.2)(dense1)  # Consider experimenting with the dropout rate\n","output_layer = keras.layers.Dense(N_CLASSES, activation='softmax')(dropout)\n","\n","model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n","\n","if DEBUG:\n","    model.summary()\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"],"metadata":{"id":"H38Z_UwRudUk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Use kfold**"],"metadata":{"id":"sG-G3wQJWOCn"}},{"cell_type":"code","source":["# Execute kfold\n","for fold, (train_index, test_index) in enumerate(kf.split(images), 1):\n","\n","    # encode labels\n","    label_encoder = skp.LabelEncoder()\n","    integer_encoded_labels = label_encoder.fit_transform(labels)\n","    one_hot_encoded_labels = keras.utils.to_categorical(integer_encoded_labels)\n","\n","    X_train, X_test = images[train_index], images[test_index]\n","    y_train, y_test = one_hot_encoded_labels[train_index], one_hot_encoded_labels[test_index]\n","\n","    print(f\"\\nFold {fold} - Training set: X_train shape = {X_train.shape}, y_train shape = {y_train.shape}\")\n","    print(f\"Fold {fold} - Testing set: X_test shape = {X_test.shape}, y_test shape = {y_test.shape}\")\n","\n","    integer_class_labels = np.argmax(y_train, axis=1)\n","\n","    # Train and test modele\n","    train_and_evaluate(model, X_train, y_train, X_test, y_test)"],"metadata":{"id":"iNujF2FVR7w7"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}