{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall umap -y\n",
    "# !pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.model_selection as skms\n",
    "import sklearn.preprocessing as skp\n",
    "import sklearn.utils as sku\n",
    "import sklearn.decomposition as skd\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_DATA_PATH = \"./herbier/data_public/words/\"\n",
    "METADATA_PATH = \"./herbier/data_public/ascii/words.txt\"\n",
    "\n",
    "IMAGE_HEIGHT = IMAGE_WIDTH = 128\n",
    "\n",
    "CLASSES = ['a01-000u', 'a01-003u']\n",
    "N_CLASSES = len(CLASSES)\n",
    "\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- cross-validation\n",
    "- \n",
    "- voir pour ignorer le fichier METADATA\n",
    "- prétraitement image\n",
    "  - réduire spectre pixels\n",
    "  - resize\n",
    "- data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words_data(data_path, metadata_path, selected_writers = []):\n",
    "    if selected_writers == []:\n",
    "        raise ValueError(\"selected_writers must be a non-empty list of writer IDs\")\n",
    "\n",
    "    data = []\n",
    "\n",
    "    with open(metadata_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if not line.startswith(\"#\"):\n",
    "                components = line.strip().split(' ')\n",
    "                word_id = components[0]\n",
    "                \n",
    "                parts = word_id.split('-')\n",
    "                writer_id = '-'.join(parts[:2])\n",
    "\n",
    "                if writer_id in selected_writers:\n",
    "                    image_subfolder = parts[0]\n",
    "                    image_filename = f\"{word_id}.png\"\n",
    "                    image_path = os.path.join(data_path, image_subfolder, writer_id, image_filename)\n",
    "                    \n",
    "                    if os.path.exists(image_path):\n",
    "                        try:\n",
    "                            img = tf.io.read_file(image_path)\n",
    "                            img = tf.image.decode_png(img)\n",
    "                            data.append({\n",
    "                                'image_path': image_path,\n",
    "                                'writer_id': writer_id,\n",
    "                                'image_array': img\n",
    "                            })\n",
    "                        except tf.errors.InvalidArgumentError:\n",
    "                            print(f\"Image not found for word ID: {word_id} at {image_path}\")\n",
    "                    else:\n",
    "                        print(f\"Image not found for word ID: {word_id} at {image_path}\")\n",
    "\n",
    "    return data\n",
    " \n",
    "words_data = load_words_data(WORD_DATA_PATH, METADATA_PATH, selected_writers=CLASSES)\n",
    "\n",
    "if DEBUG:\n",
    "  print(f\"Loaded {len(words_data)} words.\")\n",
    "  for entry in words_data[:5]:\n",
    "      print(f\"  Writer ID: {entry['writer_id']}; image shape: {entry['image_array'].shape}\")\n",
    "\n",
    "if DEBUG: \n",
    "    print(\"number of writers: \", len(set([entry['writer_id'] for entry in words_data])))\n",
    "\n",
    "if DEBUG:\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  for i in range(25):\n",
    "      plt.subplot(5, 5, i + 1)\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "      plt.grid(False)\n",
    "      plt.imshow(words_data[i]['image_array'], cmap=plt.cm.binary)\n",
    "      plt.xlabel(words_data[i]['writer_id'])\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    labels = []\n",
    "    images = []\n",
    "\n",
    "    for entry in data:\n",
    "        # Resize the image\n",
    "        img = tf.image.resize(entry['image_array'], [IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "\n",
    "        img = img.numpy().astype('float32') / 255.0  # Normalize and convert to float32\n",
    "        images.append(img)\n",
    "\n",
    "        labels.append(entry['writer_id'])\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "images, labels = preprocess_data(words_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = skms.train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"X_train: {X_train.shape}; y_train: {y_train.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}; y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode labels\n",
    "label_encoder = skp.LabelEncoder()\n",
    "integer_encoded_labels = label_encoder.fit_transform(labels)\n",
    "one_hot_encoded_labels = keras.utils.to_categorical(integer_encoded_labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = skms.train_test_split(images, one_hot_encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "integer_class_labels = np.argmax(y_train, axis=1)\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"X_train: {X_train.shape}; y_train: {y_train.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}; y_test: {y_test.shape}\")\n",
    "    # number of training samples per class\n",
    "    print(\"number of training samples per class: \", np.bincount(integer_class_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 1))\n",
    "\n",
    "# Define L1 and L2 regularization\n",
    "l1_l2 = keras.regularizers.l1_l2(l1=1e-5, l2=1e-4)\n",
    "\n",
    "# path 1\n",
    "conv1_1 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l1_l2)(input_layer)\n",
    "pool1_1 = keras.layers.MaxPooling2D((2, 2))(conv1_1)\n",
    "conv1_2 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l1_l2)(pool1_1)\n",
    "pool1_2 = keras.layers.MaxPooling2D((2, 2))(conv1_2)\n",
    "\n",
    "# path 2\n",
    "conv2_1 = keras.layers.Conv2D(32, (5, 5), activation='relu', padding='same', kernel_regularizer=l1_l2)(input_layer)\n",
    "pool2_1 = keras.layers.MaxPooling2D((2, 2))(conv2_1)\n",
    "conv2_2 = keras.layers.Conv2D(64, (5, 5), activation='relu', padding='same', kernel_regularizer=l1_l2)(pool2_1)\n",
    "pool2_2 = keras.layers.MaxPooling2D((2, 2))(conv2_2)\n",
    "\n",
    "# merge paths\n",
    "merged = keras.layers.concatenate([pool1_2, pool2_2])\n",
    "\n",
    "flat = keras.layers.Flatten()(merged)\n",
    "dense1 = keras.layers.Dense(128, activation='relu', kernel_regularizer=l1_l2)(flat)\n",
    "dropout = keras.layers.Dropout(0.5)(dense1)  # Consider experimenting with the dropout rate\n",
    "output_layer = keras.layers.Dense(N_CLASSES, activation='softmax')(dropout)\n",
    "\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8 # fine tuned\n",
    "EPOCHS = 200\n",
    "\n",
    "class_weights = sku.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(integer_class_labels),\n",
    "    y=integer_class_labels\n",
    ")\n",
    "class_weights_dict = {i : weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    epochs=EPOCHS, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    class_weight=class_weights_dict,\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = model.predict(X_train)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = skp.StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# Now, use the standardized features with UMAP\n",
    "\n",
    "def evaluate_model(n_neighbors, min_dist, ax):\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=2, metric='euclidean')\n",
    "    embedding = reducer.fit_transform(np.nan_to_num(features_standardized))\n",
    "\n",
    "    # reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=2, metric='euclidean')\n",
    "    # embedding = reducer.fit_transform(np.nan_to_num(features))\n",
    "\n",
    "    sc = ax.scatter(embedding[:, 0], embedding[:, 1],\n",
    "                    c=integer_class_labels, edgecolor='none', alpha=0.5,\n",
    "                    cmap=plt.cm.get_cmap('Accent', N_CLASSES))\n",
    "    ax.set_xlabel('UMAP component 1')\n",
    "    ax.set_ylabel('UMAP component 2')\n",
    "    ax.set_title(f'n_neighbors={n_neighbors}, min_dist={min_dist}')\n",
    "\n",
    "    if n_neighbors == n_neighbors_list[-1] and min_dist == min_dist_list[-1]:\n",
    "        plt.colorbar(sc, ax=ax)\n",
    "\n",
    "n_neighbors_list = [10, 20, 30]\n",
    "min_dist_list = [0.0, 0.1, 0.2]\n",
    "\n",
    "fig, axes = plt.subplots(len(n_neighbors_list), len(min_dist_list), figsize=(15, 12))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (n_neighbors, min_dist) in enumerate([(x, y) for x in n_neighbors_list for y in min_dist_list]):\n",
    "    evaluate_model(n_neighbors, min_dist, axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "model_predictions = np.argmax(model.predict(X_test), axis=1)\n",
    "conf_matrix = skm.confusion_matrix(true_labels, model_predictions)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cax = ax.matshow(conf_matrix, cmap=plt.cm.Blues)\n",
    "fig.colorbar(cax)\n",
    "\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "for (i, j), val in np.ndenumerate(conf_matrix):\n",
    "    ax.text(j, i, val, ha='center', va='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "model_predictions = np.argmax(model.predict(X_test), axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "cls_report = skm.classification_report(true_labels, model_predictions)\n",
    "print(f\"Classification Report:\\n{cls_report}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
