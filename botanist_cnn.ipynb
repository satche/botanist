{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQpTs-iVOHZy",
        "outputId": "64416173-fda7-4d0a-fca4-84180e8cb2f4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn.model_selection as skms\n",
        "import sklearn.preprocessing as skp\n",
        "import sklearn.utils as sku\n",
        "import sklearn.decomposition as skd\n",
        "import sklearn.metrics as skm\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "!pip install umap-learn\n",
        "import umap.umap_ as umap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeyOToxdTux3"
      },
      "source": [
        "**Settings and unzip data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU1FJAflPcim",
        "outputId": "ad6219cd-bf81-4d7e-f988-a70bbff0876a"
      },
      "outputs": [],
      "source": [
        "# GOOGLE COLAB\n",
        "USE_GOOGLE_COLAB = True # Are you using Google Colab ?\n",
        "COLAB_WORKING_PATH = \"/content/drive/My Drive/Colab/Botanist\" # Path to folder in Google Drive\n",
        "\n",
        "# Mount on Google Drive\n",
        "if USE_GOOGLE_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# PATHS\n",
        "DATASET_ZIP_PATH = f\"{COLAB_WORKING_PATH}/herbier.zip\" # Path to zipped data\n",
        "DATASET_PATH = \"/content/data/\" # Where the unzipped data should land ?\n",
        "\n",
        "WORKDIR = f\"{DATASET_PATH}herbier\"\n",
        "WORD_DATA_PATH = f\"{WORKDIR}/data_public/words/\"\n",
        "METADATA_PATH = f\"{WORKDIR}/data_public/ascii/words.txt\"\n",
        "\n",
        "# Create our data folder, unzip the data\n",
        "if not os.path.exists(WORKDIR):\n",
        "  !mkdir -p $DATASET_PATH\n",
        "  !unzip \"$DATASET_ZIP_PATH\" -d $DATASET_PATH\n",
        "\n",
        "# CNN\n",
        "IMAGE_HEIGHT = 50\n",
        "IMAGE_WIDTH = 75\n",
        "N_CLASSES = 2\n",
        "\n",
        "# Choose random classes\n",
        "all_dirs = os.listdir(WORD_DATA_PATH)\n",
        "selected_top_dirs = random.sample(all_dirs, N_CLASSES)\n",
        "sub_dirs = {top_dir: os.listdir(os.path.join(WORD_DATA_PATH, top_dir)) for top_dir in selected_top_dirs}\n",
        "random_subdirs = {top_dir: random.choice(sub_dirs[top_dir]) for top_dir in selected_top_dirs}\n",
        "\n",
        "CLASSES = list(random_subdirs.values())\n",
        "print(f\"Selected classes: {CLASSES}\")\n",
        "\n",
        "FLATTEN_LAYER_NAME = 'flattened'\n",
        "\n",
        "DEBUG = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-40IhemDTlK9"
      },
      "source": [
        "**Load data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "UKRq69r4OHZ2",
        "outputId": "18a92cef-776b-4fd4-e419-bf02d364213c"
      },
      "outputs": [],
      "source": [
        "def load_words_data(data_path, metadata_path, selected_writers = []):\n",
        "    if selected_writers == []:\n",
        "        raise ValueError(\"selected_writers must be a non-empty list of writer IDs\")\n",
        "\n",
        "    data = []\n",
        "\n",
        "    with open(metadata_path, 'r') as file:\n",
        "        for line in file:\n",
        "            if not line.startswith(\"#\"):\n",
        "                components = line.strip().split(' ')\n",
        "                word_id = components[0]\n",
        "\n",
        "                parts = word_id.split('-')\n",
        "                writer_id = '-'.join(parts[:2])\n",
        "\n",
        "                if writer_id in selected_writers:\n",
        "                    image_subfolder = parts[0]\n",
        "                    image_filename = f\"{word_id}.png\"\n",
        "                    image_path = os.path.join(data_path, image_subfolder, writer_id, image_filename)\n",
        "\n",
        "                    if os.path.exists(image_path):\n",
        "                        try:\n",
        "                            img = tf.io.read_file(image_path)\n",
        "                            img = tf.image.decode_png(img)\n",
        "                            data.append({\n",
        "                                'image_path': image_path,\n",
        "                                'writer_id': writer_id,\n",
        "                                'image_array': img\n",
        "                            })\n",
        "                        except tf.errors.InvalidArgumentError:\n",
        "                            print(f\"Image not found for word ID: {word_id} at {image_path}\")\n",
        "                    else:\n",
        "                        print(f\"Image not found for word ID: {word_id} at {image_path}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "words_data = load_words_data(WORD_DATA_PATH, METADATA_PATH, selected_writers=CLASSES)\n",
        "images = [entry['image_array'] for entry in words_data]\n",
        "labels = [entry['writer_id'] for entry in words_data]\n",
        "\n",
        "X_train, X_test, y_train, y_test = skms.train_test_split(np.array(images), np.array(labels), test_size=0.2, random_state=42)\n",
        "\n",
        "def plot_images(images, labels, num=5, class_num=N_CLASSES):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(num):\n",
        "        plt.subplot(class_num,num,i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
        "        plt.xlabel(labels[i])\n",
        "    plt.show()\n",
        "\n",
        "def plot_random_samples(classes, num=5):\n",
        "  for class_name in classes:\n",
        "      class_indices = np.where(y_train == class_name)[0]\n",
        "      size = min(num, len(class_indices))\n",
        "      random_indices = np.random.choice(class_indices, size=size, replace=False)\n",
        "      random_images = X_train[random_indices]\n",
        "      plot_images(random_images, [class_name] * num)\n",
        "\n",
        "\n",
        "if DEBUG:\n",
        "  print(f\"Loaded {len(words_data)} words.\")\n",
        "  for entry in words_data[:5]:\n",
        "      print(f\"  Writer ID: {entry['writer_id']}; image shape: {entry['image_array'].shape}\")\n",
        "\n",
        "  print(\"number of writers: \", len(set([entry['writer_id'] for entry in words_data])))\n",
        "\n",
        "  plot_random_samples(CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL1b3PvLTY5B"
      },
      "source": [
        "**Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "JeRayEfpOHZ3",
        "outputId": "61fc9f35-7d93-43fb-aef4-c7c55d6cebd7"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data):\n",
        "    labels = []\n",
        "    images = []\n",
        "\n",
        "    for entry in data:\n",
        "        # Resize the image while preserving aspect ratio\n",
        "        img = np.array(entry['image_array'])\n",
        "        old_size = img.shape[:2]\n",
        "\n",
        "        ratio = float(IMAGE_HEIGHT)/old_size[0]\n",
        "        new_size = tuple([int(x*ratio) for x in old_size])\n",
        "\n",
        "        img = cv2.resize(img, (new_size[1], new_size[0]))\n",
        "\n",
        "        # Ignore images that are too narrows\n",
        "        if new_size[1] < IMAGE_WIDTH:\n",
        "          continue;\n",
        "\n",
        "        # Crop images that are too wide\n",
        "        if new_size[1] > IMAGE_WIDTH:\n",
        "            start_x = (new_size[1] - IMAGE_WIDTH) // 2\n",
        "            img = img[:, start_x:start_x + IMAGE_WIDTH]\n",
        "            new_size = (new_size[0], IMAGE_WIDTH)\n",
        "\n",
        "        img = img.astype('float32') / 255.0\n",
        "\n",
        "        # Ensure dimensions format is correct: (sample_n, width, height, channels)\n",
        "        img = np.expand_dims(img, axis=-1)\n",
        "        delta_w = IMAGE_WIDTH - new_size[1]\n",
        "        delta_h = IMAGE_HEIGHT - img.shape[0]\n",
        "        delta_w = IMAGE_WIDTH - img.shape[1]\n",
        "        padding = ((0, delta_h), (0, delta_w), (0, 0))\n",
        "        img = np.pad(img, padding, 'constant')\n",
        "\n",
        "        images.append(img)\n",
        "        labels.append(entry['writer_id'])\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "\n",
        "images, labels = preprocess_data(words_data)\n",
        "X_train, X_test, y_train, y_test = skms.train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "if DEBUG:\n",
        "  print(f\"X_train: {X_train.shape}; y_train: {y_train.shape}\")\n",
        "  print(f\"X_test: {X_test.shape}; y_test: {y_test.shape}\")\n",
        "  plot_random_samples(CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbSxp-eGOHZ3"
      },
      "outputs": [],
      "source": [
        "# data augmentation\n",
        "\n",
        "data_generator = keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=False,\n",
        "    fill_mode='nearest'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5K7LbvLTgbB"
      },
      "source": [
        "**Kfold**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPcfiBsVRxG_"
      },
      "outputs": [],
      "source": [
        "# Define number of splits\n",
        "n_splits = 5\n",
        "\n",
        "# Create Kfold instance\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLk6ZBvzWKd3"
      },
      "source": [
        "**Set function to use kfold**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2--w-RnTsGl"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(model, X_train, y_train, X_test, y_test):\n",
        "  BATCH_SIZE = 5 # fine tuned\n",
        "  EPOCHS = 30\n",
        "\n",
        "  train_generator = data_generator.flow(X_train, y_train, batch_size=BATCH_SIZE)\n",
        "\n",
        "  class_weights = sku.compute_class_weight(\n",
        "      class_weight='balanced',\n",
        "      classes=np.unique(integer_class_labels),\n",
        "      y=integer_class_labels\n",
        "  )\n",
        "  class_weights_dict = {i : weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "  early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "  history = model.fit(\n",
        "      train_generator,\n",
        "      epochs=EPOCHS,\n",
        "      steps_per_epoch=len(X_train) // BATCH_SIZE,  # Number of batches per epoch\n",
        "      validation_data=(X_test, y_test),\n",
        "      class_weight=class_weights_dict,\n",
        "      callbacks=[early_stopping]\n",
        "  )\n",
        "\n",
        "  ############\n",
        "\n",
        "  feature_layer = model.get_layer(FLATTEN_LAYER_NAME).output\n",
        "\n",
        "  # Create a feature extractor model\n",
        "  feature_extractor_model = keras.models.Model(inputs=model.input, outputs=feature_layer)\n",
        "\n",
        "  # Now you can use this model to extract features\n",
        "  features = feature_extractor_model.predict(X_train)\n",
        "\n",
        "  ############\n",
        "\n",
        "  # features = model.predict(X_train)\n",
        "\n",
        "  if DEBUG:\n",
        "      print(f\"features shape: {features.shape}\")\n",
        "\n",
        "  # Standardize the features\n",
        "  scaler = skp.StandardScaler()\n",
        "  features_standardized = scaler.fit_transform(features)\n",
        "\n",
        "  # Now, use the standardized features with UMAP\n",
        "\n",
        "  def evaluate_model(n_neighbors, min_dist, ax):\n",
        "      reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=32, metric='euclidean')\n",
        "      embedding = reducer.fit_transform(np.nan_to_num(features_standardized))\n",
        "\n",
        "      sc = ax.scatter(embedding[:, 0], embedding[:, 1],\n",
        "                      c=integer_class_labels, edgecolor='none', alpha=0.5,\n",
        "                      cmap=plt.cm.get_cmap('Accent', N_CLASSES))\n",
        "      ax.set_xlabel('UMAP component 1')\n",
        "      ax.set_ylabel('UMAP component 2')\n",
        "      ax.set_title(f'n_neighbors={n_neighbors}, min_dist={min_dist}')\n",
        "\n",
        "      if n_neighbors == n_neighbors_list[-1] and min_dist == min_dist_list[-1]:\n",
        "          plt.colorbar(sc, ax=ax)\n",
        "\n",
        "  n_neighbors_list = [10, 20, 30]\n",
        "  min_dist_list = [0.0, 0.1, 0.2]\n",
        "\n",
        "  fig, axes = plt.subplots(len(n_neighbors_list), len(min_dist_list), figsize=(15, 12))\n",
        "\n",
        "  axes = axes.flatten()\n",
        "\n",
        "  for idx, (n_neighbors, min_dist) in enumerate([(x, y) for x in n_neighbors_list for y in min_dist_list]):\n",
        "      evaluate_model(n_neighbors, min_dist, axes[idx])\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eygzGbC1ud1n"
      },
      "source": [
        "**Define model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H38Z_UwRudUk",
        "outputId": "3d3740e0-44b5-41b1-e998-6f672709546b"
      },
      "outputs": [],
      "source": [
        "# Define modele\n",
        "input_layer = keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 1))\n",
        "\n",
        "# Define L1 and L2 regularization\n",
        "l1_l2 = keras.regularizers.l1_l2(l1=0, l2=1e-4)\n",
        "\n",
        "# path 1\n",
        "conv1_1 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l1_l2)(input_layer)\n",
        "pool1_1 = keras.layers.MaxPooling2D((2, 2))(conv1_1)\n",
        "conv1_2 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l1_l2)(pool1_1)\n",
        "pool1_2 = keras.layers.MaxPooling2D((2, 2))(conv1_2)\n",
        "\n",
        "# path 2\n",
        "conv2_1 = keras.layers.Conv2D(32, (5, 5), activation='relu', padding='same', kernel_regularizer=l1_l2)(input_layer)\n",
        "pool2_1 = keras.layers.MaxPooling2D((2, 2))(conv2_1)\n",
        "conv2_2 = keras.layers.Conv2D(64, (5, 5), activation='relu', padding='same', kernel_regularizer=l1_l2)(pool2_1)\n",
        "pool2_2 = keras.layers.MaxPooling2D((2, 2))(conv2_2)\n",
        "\n",
        "# merge paths\n",
        "merged = keras.layers.concatenate([pool1_2, pool2_2])\n",
        "\n",
        "flat = keras.layers.Flatten()(merged)\n",
        "dense1 = keras.layers.Dense(128, activation='relu', kernel_regularizer=l1_l2, name=FLATTEN_LAYER_NAME)(flat)\n",
        "dropout = keras.layers.Dropout(0.2)(dense1)  # Consider experimenting with the dropout rate\n",
        "output_layer = keras.layers.Dense(N_CLASSES, activation='softmax')(dropout)\n",
        "\n",
        "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "if DEBUG:\n",
        "    model.summary()\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG-G3wQJWOCn"
      },
      "source": [
        "**Use kfold**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iNujF2FVR7w7",
        "outputId": "663993bc-76fe-4320-8d5e-37a949d14d0c"
      },
      "outputs": [],
      "source": [
        "# Execute kfold\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(images), 1):\n",
        "\n",
        "    # encode labels\n",
        "    label_encoder = skp.LabelEncoder()\n",
        "    integer_encoded_labels = label_encoder.fit_transform(labels)\n",
        "    one_hot_encoded_labels = keras.utils.to_categorical(integer_encoded_labels)\n",
        "\n",
        "    X_train, X_test = images[train_index], images[test_index]\n",
        "    y_train, y_test = one_hot_encoded_labels[train_index], one_hot_encoded_labels[test_index]\n",
        "\n",
        "    print(f\"\\nFold {fold} - Training set: X_train shape = {X_train.shape}, y_train shape = {y_train.shape}\")\n",
        "    print(f\"Fold {fold} - Testing set: X_test shape = {X_test.shape}, y_test shape = {y_test.shape}\")\n",
        "\n",
        "    integer_class_labels = np.argmax(y_train, axis=1)\n",
        "\n",
        "    # Train and test modele\n",
        "    train_and_evaluate(model, X_train, y_train, X_test, y_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
